{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl6cZVur2HPT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import sqlite3\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "LOCAL_FILE = \"data/local_data.csv\"\n",
        "API_URL = \"https://api.publicapis.org/entries\"  # Example API\n",
        "\n",
        "# Function to fetch remote data (API)\n",
        "def fetch_remote_data(api_url):\n",
        "    try:\n",
        "        response = requests.get(api_url)\n",
        "        response.raise_for_status()  # Raise error for bad response\n",
        "        data = response.json()\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to load local CSV or JSON\n",
        "def load_local_data(file_path):\n",
        "    try:\n",
        "        if file_path.endswith(\".csv\"):\n",
        "            return pd.read_csv(file_path)\n",
        "        elif file_path.endswith(\".json\"):\n",
        "            return pd.read_json(file_path)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading local data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to transform data (convert format, modify columns)\n",
        "def transform_data(df, output_format=\"csv\"):\n",
        "    try:\n",
        "        df[\"new_column\"] = \"Sample Data\"  # Example transformation\n",
        "        if output_format == \"csv\":\n",
        "            df.to_csv(\"output/transformed_data.csv\", index=False)\n",
        "        elif output_format == \"json\":\n",
        "            df.to_json(\"output/transformed_data.json\", orient=\"records\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error during transformation: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to store data in SQLite\n",
        "def store_in_database(df, db_name=\"data_store.db\", table_name=\"dataset\"):\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_name)\n",
        "        df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
        "        conn.close()\n",
        "        print(f\"Data stored successfully in {db_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error storing data: {e}\")\n",
        "\n",
        "# Function to summarize data\n",
        "def summarize_data(df):\n",
        "    try:\n",
        "        summary = {\n",
        "            \"num_records\": len(df),\n",
        "            \"num_columns\": len(df.columns),\n",
        "            \"columns\": list(df.columns),\n",
        "        }\n",
        "        print(json.dumps(summary, indent=4))\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing data: {e}\")\n",
        "\n",
        "# Main ETL process\n",
        "def run_etl():\n",
        "    print(\"Starting ETL process...\")\n",
        "\n",
        "    # Extract\n",
        "    print(\"Fetching remote data...\")\n",
        "    remote_data = fetch_remote_data(API_URL)\n",
        "    if remote_data:\n",
        "        remote_df = pd.DataFrame(remote_data[\"entries\"])  # Example structure\n",
        "\n",
        "    print(\"Loading local data...\")\n",
        "    local_df = load_local_data(LOCAL_FILE)\n",
        "\n",
        "    # Transform\n",
        "    if local_df is not None:\n",
        "        print(\"Transforming local data...\")\n",
        "        transformed_df = transform_data(local_df)\n",
        "\n",
        "        # Merge example\n",
        "        if remote_data:\n",
        "            merged_df = pd.concat([transformed_df, remote_df], axis=0, ignore_index=True)\n",
        "\n",
        "            # Summarize\n",
        "            print(\"Summary of merged data:\")\n",
        "            summarize_data(merged_df)\n",
        "\n",
        "            # Load to database\n",
        "            print(\"Storing data in database...\")\n",
        "            store_in_database(merged_df)\n",
        "\n",
        "    print(\"ETL process completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_etl()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# Create and initialize database\n",
        "def create_database(db_name=\"data_store.db\"):\n",
        "    conn = sqlite3.connect(db_name)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Example table structure\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS dataset (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            column1 TEXT,\n",
        "            column2 TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"Database {db_name} initialized.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_database()\n"
      ],
      "metadata": {
        "id": "syyS_q8v2NHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}